{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the jobs of PREFER are completed and the session_xxx-xxx folders containing one folder results for each molecular representation have been created, one can evaluate the performances of the model on different bootstrapped datasets of the original test set. The performance of a simple Random Forest will be also collected on the same test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "%load_ext autoreload\n",
    "# path to the main directory\n",
    "path_to_PREFER = 'path_to/PREFER/'\n",
    "# path to submodules\n",
    "path_to_cddd = 'path_to/model_based_representations/models/cddd/'\n",
    "path_to_moler = 'path_to/model_based_representations/models/molecule-generation/'\n",
    "sys.path.append(path_to_PREFER)\n",
    "sys.path.append(path_to_cddd)\n",
    "sys.path.append(path_to_moler)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from prefer.utils.filtering import *\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from prefer.utils.post_processing_and_optimization_helpers import create_heat_map\n",
    "from prefer.utils.automation import merge_table_metrics, data_preparation, generate_molecular_representations, run, create_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set folders where the trained models are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The session_ folders contain one folder for each molecular representation\n",
    "publiclogD = f'path_to/session_xxx-xxx'\n",
    "publicsolubility = f'path_to/session_yyy-yyy'\n",
    "\n",
    "problem_types = ['regression', 'classification']\n",
    "assays = [publiclogD, publicsolubility]\n",
    "assays_names = ['publicLogD',  'publicSolubility']\n",
    "split_types = ['random',  'random']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "def convert(df, repr_name):\n",
    "    import ast\n",
    "    prova = df[repr_name].values\n",
    "    X = []\n",
    "    for index, elem in enumerate(prova):\n",
    "\n",
    "        conv_elem = re.sub(\"\\s+\", \",\", elem.strip())\n",
    "        conv_elem = conv_elem.replace(',]', ']')\n",
    "        conv_elem = conv_elem.replace('[,','[')\n",
    "        conv = ast.literal_eval(conv_elem)\n",
    "        X.append(conv)\n",
    "    X = np.array(X)\n",
    "    if 'Property_2' not in df.columns.values:\n",
    "        y = df['Property_1'].to_numpy()\n",
    "    else:  # multitasking\n",
    "        if('Property_1' not in df):\n",
    "            if('true_label_1' in df):\n",
    "                df.rename(columns = {'true_label_1': 'Property_1'}, inplace = True)\n",
    "            else:\n",
    "                raise ValueError('Promens with columns')\n",
    "            \n",
    "        y = df[['Property_1', 'Property_2']].values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def bootstrap(X, y, seed, perc = 0.8):\n",
    "    x = list(range(X.shape[0]))\n",
    "    random.Random(seed).shuffle(x)\n",
    "    limit = int(len(x) * perc)\n",
    "    index = x[:limit]\n",
    "    return X[index,:], y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_deltaAUPRC(labels, predictions, y_train):\n",
    "    from sklearn.metrics import precision_recall_curve, auc\n",
    "    precision, recall, _ = precision_recall_curve(labels, predictions)\n",
    "    auc_score = auc(recall, precision)\n",
    "    deltaAUPRC = round(auc_score - (np.sum(y_train) / len(y_train)), 3)\n",
    "    return deltaAUPRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc_auc(labels, predictions):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    ROC_AUC = round(roc_auc_score(labels, predictions), 3)\n",
    "    return ROC_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "def baseline(problem_type, Xtrain, ytrain, Xtest, ytest):\n",
    "\n",
    "    if problem_type == 'regression':\n",
    "        metric = 'R2'\n",
    "        rf = RandomForestRegressor(max_depth=20, n_estimators = 100, random_state=0)\n",
    "        rf.fit(Xtrain, ytrain)\n",
    "        predictions = rf.predict(Xtest)\n",
    "    elif problem_type == 'classification':\n",
    "        metric = 'kappa'\n",
    "        rf = RandomForestClassifier(max_depth=20, n_estimators = 100, random_state=0)\n",
    "        rf.fit(Xtrain, ytrain)\n",
    "        predictions = rf.predict_proba(Xtest)[:, 1]\n",
    "        #from sklearn.metrics import roc_auc_score\n",
    "        #RF_baseline_performance = roc_auc_score(ytest, predictions)\n",
    "    print(Xtest_new.shape)\n",
    "    print(predictions.shape)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performances(ytest, predictions, ytrain, problem_type):\n",
    "    from sklearn.metrics import r2_score\n",
    "    num_tasks = ytest.ndim\n",
    "\n",
    "    if problem_type == 'regression':\n",
    "        if(num_tasks == 1):\n",
    "            performance = r2_score(ytest, predictions)\n",
    "        else:\n",
    "            performance = []\n",
    "            for task in range(0, num_tasks):\n",
    "                performance.append(r2_score(ytest[:,task], predictions[:, task]))\n",
    "    elif problem_type == 'classification':\n",
    "        performance = compute_roc_auc(ytest, predictions)\n",
    "    return performance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefer.molecule_representations.fingerprints_representations_builder import (\n",
    "    FingerprintsRepresentationsBuilder,\n",
    ")\n",
    "\n",
    "from prefer.molecule_representations.descriptors2D_representations_builder import (\n",
    "    Descriptors2DRepresentationsBuilder,\n",
    ")\n",
    "\n",
    "def compute_representation_again(df, split_type, repr_name):\n",
    "    df1 = df.copy()\n",
    "    if split_type == 'temporal':\n",
    "        df1['Time'] = pd.to_datetime(df1['Time'])\n",
    "    if(repr_name == 'FINGERPRINTS'):\n",
    "        df1 = df1.drop(columns = ['FINGERPRINTS', 'is_train'])\n",
    "        fing_representation = FingerprintsRepresentationsBuilder()\n",
    "        fingerprints = fing_representation.build_representations(df1, split_type=split_type)\n",
    "        Xtrain, ytrain, Xtest, ytest, index_train, index_test = fingerprints.split(return_indices = True)\n",
    "    elif(repr_name == 'DESCRIPTORS2D'):\n",
    "        df1 = df1.drop(columns = ['DESCRIPTORS2D', 'is_train'])\n",
    "        _2d_descriptors = Descriptors2DRepresentationsBuilder()\n",
    "        _2dd = _2d_descriptors.build_representations(df1, split_type=split_type)\n",
    "        Xtrain, ytrain, Xtest, ytest, index_train, index_test = _2dd.split(return_indices = True)\n",
    "    else:\n",
    "        raise ValueError(f'representation neither FINGERPRINTS neither 2DD')\n",
    "        \n",
    "    return Xtrain, ytrain, Xtest, ytest, index_train, index_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefer.utils.features_scaling import apply_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Retrieve Bench object\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "autosklearn_performances = dict()\n",
    "RF_performances = dict()\n",
    "for problem_type,assay, assays_name, split_type in zip(problem_types, assays, assays_names, split_types):\n",
    "    try:\n",
    "        print(assay)\n",
    "        print(assays_name)\n",
    "        autosklearn_performances[assays_name] = dict()\n",
    "        RF_performances[assays_name] = []\n",
    "        repr_folders = os.listdir(assay)\n",
    "        for repr_folder in repr_folders:\n",
    "            if((not repr_folder.startswith('_')) and (not repr_folder.startswith('.'))):\n",
    "                print(repr_folder)\n",
    "                current_representation = repr_folder.split('_')[0]\n",
    "                autosklearn_performances[assays_name][current_representation]=[]\n",
    "                print(current_representation)\n",
    "                path_bench = f'{assay}/{repr_folder}/bench.pkl'\n",
    "                # import bench\n",
    "                import pickle\n",
    "                with open(path_bench, 'rb') as f:\n",
    "                    bench = pickle.load(f)\n",
    "\n",
    "                # take Xtest and ytest\n",
    "                if('Property_2' in bench['df'][current_representation].columns.values):\n",
    "                    if('true_label_1' in bench['df'][current_representation].columns.values):\n",
    "                        bench['df'][current_representation].rename(columns={'true_label_1': 'Property_1'}, inplace = True)\n",
    "                    ytest = bench['df'][current_representation][bench['df'][current_representation].is_train == False][['Property_1', 'Property_2']].values\n",
    "                    ytrain = bench['df'][current_representation][bench['df'][current_representation].is_train == True][['Property_1', 'Property_2']].values\n",
    "                else:\n",
    "                    ytest = bench['df'][current_representation][bench['df'][current_representation].is_train == False].Property_1.values\n",
    "                    ytrain = bench['df'][current_representation][bench['df'][current_representation].is_train == True].Property_1.values\n",
    "                Xtest = np.stack(bench['df'][current_representation][bench['df'][current_representation].is_train == False][current_representation].tolist())\n",
    "                Xtrain = np.stack(bench['df'][current_representation][bench['df'][current_representation].is_train == True][current_representation].tolist())\n",
    "     \n",
    "                featuriz = bench['features_scaling_type'][current_representation]\n",
    "                print(f'>>>>>{featuriz}')\n",
    "                if bench['features_scaling_type'][current_representation] is not None:\n",
    "                    print('FEATURIZATION NEEDED')\n",
    "                    # Scale features before prediction.\n",
    "                    Xtest = [\n",
    "                        apply_scaling(\n",
    "                            features_vect=x,\n",
    "                            scaling_type=bench['features_scaling_type'][current_representation],\n",
    "                            means=bench['features_means_vect'][current_representation],\n",
    "                            stds=bench['features_stds_vect'][current_representation],\n",
    "                        )\n",
    "                        for x in Xtest\n",
    "                    ]\n",
    "                    Xtest = np.array(Xtest)\n",
    "                    # Scale features before prediction.\n",
    "                    Xtrain = [\n",
    "                        apply_scaling(\n",
    "                            features_vect=x,\n",
    "                            scaling_type=bench['features_scaling_type'][current_representation],\n",
    "                            means=bench['features_means_vect'][current_representation],\n",
    "                            stds=bench['features_stds_vect'][current_representation],\n",
    "                        )\n",
    "                        for x in Xtrain\n",
    "                    ]\n",
    "                    Xtrain = np.array(Xtrain)\n",
    "                \n",
    "                # bootstrap\n",
    "                seeds = [0, 1, 2, 3, 4, 5]\n",
    "                for seed in seeds:\n",
    "                    print(f'Current seed: {seed}')\n",
    "                    Xtest_new, ytest_new = bootstrap(Xtest, ytest, seed, perc = 0.8)\n",
    "                    print(f'autosklearn_predictions')\n",
    "                    if problem_type == 'classification':\n",
    "                        autosklearn_predictions = bench['best_estimator'][current_representation].predict_proba(Xtest_new)[:, 1]\n",
    "                    else:\n",
    "                        autosklearn_predictions = bench['best_estimator'][current_representation].predict(Xtest_new)\n",
    "                    print(f'RF_predictions')\n",
    "                    RF_predictions = baseline(problem_type, Xtrain, ytrain, Xtest_new, ytest_new)\n",
    "                    print(f'autosklearn_performances')\n",
    "                    perf = performances(ytest_new, autosklearn_predictions, ytrain, problem_type)\n",
    "                    autosklearn_performances[assays_name][current_representation].append(perf)\n",
    "                    print(f'RF_performances')\n",
    "                    perf = performances(ytest_new, RF_predictions, ytrain, problem_type)\n",
    "                    RF_performances[assays_name].append(perf)\n",
    "    except Exception as e:\n",
    "        print(f'problem with {assays_name} in particular {e}')\n",
    "        continue\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict = dict()\n",
    "final_dict['RF'] = RF_performances\n",
    "final_dict['autosklearn'] = autosklearn_performances\n",
    "name = str(assays_names)\n",
    "with open(f'final_dict_{name}.pickle', 'wb') as handle:\n",
    "    pickle.dump(final_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (prefer-env-released2)",
   "language": "python",
   "name": "prefer-env-released2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
