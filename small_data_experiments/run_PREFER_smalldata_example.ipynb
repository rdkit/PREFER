{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run PREFER for [FS-Mol](https://github.com/microsoft/FS-Mol) test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook can be used to run PREFER on the FS-Mol test data, after extracting the data using the \"extract_zipped_files.ipynb\" notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WARNING:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this notebook, please \n",
    "1) use the prefer-environment\n",
    "2) unpack the git submodules within the PREFER repo as described in the README.txt\n",
    "3) Change the config files as described in the README.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "# path to the main directory\n",
    "path_to_PREFER = 'path_to/PREFER/'\n",
    "# path to submodules\n",
    "path_to_cddd = 'path_to/cddd/'\n",
    "path_to_moler = 'path_to/molecule-generation/'\n",
    "sys.path.append(path_to_PREFER)\n",
    "sys.path.append(path_to_cddd)\n",
    "sys.path.append(path_to_moler)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from prefer.utils.filtering import *\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from prefer.utils.post_processing_and_optimization_helpers import create_heat_map\n",
    "from prefer.utils.automation import merge_table_metrics, data_preparation, generate_molecular_representations, run, create_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPARING PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the shell commands to compute the data based molecular representations\n",
    "path_to_cddd_model = 'path_to/cddd/default_model'\n",
    "path_to_moler_model = 'path_to/cddd/default_model'\n",
    "path_to_compute_model_based_representations = '../compute_model_based_representations.py' # path to the python script compute_model_based_representations.py\n",
    "folder_path_csv = None # here inser the path to the folder where the csv files generated from the exctract_zipped_files.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTILS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "# write to the config zaml file a new name for the path_to_df\n",
    "def set_file(prefer_args, file_name, limit_def):\n",
    "    a_yaml_file = open(prefer_args)\n",
    "    parsed_yaml_file = yaml.load(a_yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "    parsed_yaml_file['path_to_df'] =folder_path_csv+file_name\n",
    "    parsed_yaml_file['experiment_name'] =f'small_data_{limit_def}'\n",
    "    parsed_yaml_file['limit_def'] =limit_def\n",
    "\n",
    "    with open(prefer_args, 'w') as f:\n",
    "        yaml.dump(parsed_yaml_file, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from prefer.molecule_representations.fingerprints_representations_builder import (\n",
    "    FingerprintsRepresentationsBuilder,\n",
    ")\n",
    "\n",
    "from prefer.src.vector_molecule_representations import VectorMoleculeRepresentations\n",
    "from prefer.molecule_representations.descriptors2D_representations_builder import (\n",
    "    Descriptors2DRepresentationsBuilder,\n",
    ")\n",
    "\n",
    "\n",
    "def PREFER_job_smalldata(data_info, limit_def):\n",
    "    df = data_preparation(data_info)\n",
    "    split_type = data_info['split_type']\n",
    "    experiment_name = data_info['experiment_name']\n",
    "    list_of_model_based_representations_paths = data_info['list_of_model_based_representations_paths']\n",
    "    # If time split you need to provide time column name\n",
    "    temporal_info_column_name = data_info['temporal_info_column_name']\n",
    "    if (data_info['split_type'] == 'temporal'):\n",
    "        if(not temporal_info_column_name):\n",
    "            raise ValueError(f'ERROR: if time split is required then you need to provide the temporal_info_column_name')\n",
    "    # 2DD\n",
    "    _2d_descriptors = Descriptors2DRepresentationsBuilder(limit_def = limit_def)\n",
    "    _2dd = _2d_descriptors.build_representations(df, split_type=split_type)\n",
    "    _2dd.experiment_name = experiment_name\n",
    "    # FINGERPRINTS\n",
    "    fingerprints_descriptors = FingerprintsRepresentationsBuilder(limit_def = limit_def)\n",
    "    fingerprints = fingerprints_descriptors.build_representations(df, split_type=split_type)\n",
    "    fingerprints.experiment_name = experiment_name\n",
    "    \n",
    "    dict_of_representations = dict()\n",
    "    dict_of_representations[\"FINGERPRINTS\"] = fingerprints\n",
    "    dict_of_representations[\"2DDESCRIPTORS\"] = _2dd\n",
    "    import pandas as pd\n",
    "\n",
    "    if list_of_model_based_representations_paths:\n",
    "        for path in list_of_model_based_representations_paths:\n",
    "            model_name = path.split(\"_\")[0]\n",
    "\n",
    "            model_name = model_name.replace(\".\", \"\")\n",
    "            model_name = model_name.replace(\"/\", \"\")\n",
    "            vector_repr = VectorMoleculeRepresentations(\n",
    "                df=pd.DataFrame(), representation_name=\"\", split_type=\" \", limit_def = limit_def\n",
    "            )\n",
    "            model_based_representation = vector_repr.load(path)\n",
    "            model_based_representation.experiment_name = experiment_name\n",
    "            model_based_representation.representation_name = model_name\n",
    "            model_based_representation.split_type = split_type\n",
    "\n",
    "            dict_of_representations[model_name] = model_based_representation\n",
    "    representations = dict_of_representations\n",
    "    # Run PREFER\n",
    "    bench_list, dir_destination = run(representations, problem_type = data_info['problem_type'], model_instance = data_info['model_instance'])\n",
    "    # Evaluate results\n",
    "    merged = merge_table_metrics(bench_list)\n",
    "    merged.to_csv('merged.csv')\n",
    "    experiments_dict, tmp_dict = create_comparison_table(merged, metric_classification = \"deltaAUPRC\")\n",
    "    create_heat_map(experiments_dict, tmp_dict)\n",
    "    return bench_list, merged, dir_destination\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_prefer_all(data_info, limit_def, prefer_args):\n",
    "    # Preparing the shell commands to compute the data based molecular representations\n",
    "    model_name = 'CDDD'\n",
    "    run_commands = f'conda activate cddd-env-prefer-light; PYTHONPATH=\"{path_to_cddd}:{path_to_moler}:{path_to_PREFER}:$PYTHONPATH\"; export PYTHONPATH; python {path_to_compute_model_based_representations} --prefer_args {prefer_args} --path_to_model {path_to_cddd_model} --model_name {model_name}'\n",
    "    !{run_commands}\n",
    "    import datetime\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    cdddpath = f'./{model_name}_representations_{experiment_name}'\n",
    "    files = [f for f in listdir(cdddpath) if isfile(join(cdddpath, f))]\n",
    "    collect_dates = []\n",
    "    mapping = {}\n",
    "    for file in files:\n",
    "        date = file.split('_')[-1]\n",
    "        date = date.replace('.pkl','')\n",
    "        date = datetime.datetime.strptime(date, '%Y%m%d-%H%M%S')\n",
    "        collect_dates.append(date)\n",
    "        mapping[date] = file\n",
    "\n",
    "    collect_dates.sort()\n",
    "    data_info['list_of_model_based_representations_paths'].append(f'{cdddpath}/{mapping[collect_dates[-1]]}')\n",
    "    model_name = 'MOLER'\n",
    "    run_commands = f'conda activate moler-env-prefer-light; PYTHONPATH=\"{path_to_cddd}:{path_to_moler}:{path_to_PREFER}:$PYTHONPATH\"; export PYTHONPATH; python {path_to_compute_model_based_representations} --prefer_args {prefer_args} --path_to_model {path_to_moler_model} --model_name {model_name}'\n",
    "    !{run_commands}\n",
    "    # find path to the new MOLER representation found\n",
    "    import datetime\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    molerpath = f'./{model_name}_representations_{experiment_name}'\n",
    "    files = [f for f in listdir(molerpath) if isfile(join(molerpath, f))]\n",
    "    collect_dates = []\n",
    "    mapping = {}\n",
    "    for file in files:\n",
    "        date = file.split('_')[-1]\n",
    "        date = date.replace('.pkl','')\n",
    "        date = datetime.datetime.strptime(date, '%Y%m%d-%H%M%S')\n",
    "        collect_dates.append(date)\n",
    "        mapping[date] = file\n",
    "\n",
    "    collect_dates.sort()\n",
    "    data_info['list_of_model_based_representations_paths'].append(f'{molerpath}/{mapping[collect_dates[-1]]}')\n",
    "    \n",
    "    \n",
    "    _, merged, dir_destination = PREFER_job_smalldata(data_info, limit_def)\n",
    "    # save merged\n",
    "    if (not dir_destination.endswith('/')):\n",
    "        dir_destination = dir_destination+'/'\n",
    "    name = data_info['experiment_name']\n",
    "    file_name = data_info['path_to_data'].split('/')[-1]\n",
    "    file_name = file_name.replace('.csv', '')\n",
    "\n",
    "    merged.to_csv(f'{dir_destination}_merged_autosklearn_res_{name}_{file_name}.csv')\n",
    "    \n",
    "    # collect all the final merged table in one folder\n",
    "    path = f\"merged_folder_limit_def_{limit_def}\"\n",
    "\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % path)\n",
    "\n",
    "    exp_name = data_info['experiment_name']\n",
    "    merged.to_csv(f'{path}/merged_autosklearn_res_{exp_name}_{file_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read yaml config file to set the data_info\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "\n",
    "prefer_args = f'../config_files/config_PREFER_smalldata.yaml'# path to your yaml file. An example for 16 sample is stored in the config_files folder\n",
    "a_yaml_file = open(prefer_args)\n",
    "parsed_yaml_file = yaml.load(a_yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "path_to_dfs = folder_path_csv\n",
    "experiment_name = parsed_yaml_file[\"experiment_name\"]\n",
    "id_column_name = parsed_yaml_file[\"id_column_name\"]\n",
    "smiles_column_name = parsed_yaml_file[\"smiles_column_name\"]\n",
    "properties_column_name = parsed_yaml_file[\"properties_column_name_list\"]\n",
    "problem_type = parsed_yaml_file[\"problem_type\"]\n",
    "splitting_strategy = parsed_yaml_file[\"splitting_strategy\"]\n",
    "\n",
    "if 'model_instance' in parsed_yaml_file:\n",
    "    model_instance = parsed_yaml_file[\"model_instance\"]\n",
    "else:\n",
    "    model_instance = None\n",
    "\n",
    "if \"temporal_info_column_name\" in parsed_yaml_file:\n",
    "    temporal_info_column_name = parsed_yaml_file[\"temporal_info_column_name\"]\n",
    "else:\n",
    "    temporal_info_column_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    path_to_df_list = os.listdir(path_to_dfs)\n",
    "except:\n",
    "    path_to_dfs = path_to_dfs.split('/')[:-1]\n",
    "    path_to_dfs = \"/\".join(path_to_dfs)\n",
    "    path_to_df_list = os.listdir(path_to_dfs)\n",
    "    \n",
    "if(not path_to_dfs.endswith('/')):\n",
    "    path_to_dfs = path_to_dfs+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only a subset of the entire set of assays has been used for comparison\n",
    "common_assays = ['CHEMBL1243967',\n",
    " 'CHEMBL1613800',\n",
    " 'CHEMBL1613898',\n",
    " 'CHEMBL1614027',\n",
    " 'CHEMBL1614503',\n",
    " 'CHEMBL1738395',\n",
    " 'CHEMBL1738579',\n",
    " 'CHEMBL1963715',\n",
    " 'CHEMBL1963756',\n",
    " 'CHEMBL1963824',\n",
    " 'CHEMBL1963827',\n",
    " 'CHEMBL1963969',\n",
    " 'CHEMBL2218957',\n",
    " 'CHEMBL2218989',\n",
    " 'CHEMBL2219050',\n",
    " 'CHEMBL2219070',\n",
    " 'CHEMBL2219102',\n",
    " 'CHEMBL2219104',\n",
    " 'CHEMBL2219113',\n",
    " 'CHEMBL2219115',\n",
    " 'CHEMBL2219146',\n",
    " 'CHEMBL2219159',\n",
    " 'CHEMBL2219180',\n",
    " 'CHEMBL2219194',\n",
    " 'CHEMBL2219203',\n",
    " 'CHEMBL2219211',\n",
    " 'CHEMBL2219242',\n",
    " 'CHEMBL2219244',\n",
    " 'CHEMBL2219283',\n",
    " 'CHEMBL2219297',\n",
    " 'CHEMBL2219308',\n",
    " 'CHEMBL2219363',\n",
    " 'CHEMBL3214944',\n",
    " 'CHEMBL3431932',\n",
    " 'CHEMBL3431933',\n",
    " 'CHEMBL3706128',\n",
    " 'CHEMBL3707783',\n",
    " 'CHEMBL641707',\n",
    " 'CHEMBL657032',\n",
    " 'CHEMBL819742']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits_def = [16, 32, 64, 128, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for limit_def in limits_def:\n",
    "    print(f'>>>>>>> CURRENT NUMBER OF TRAINING SAMPLES: {limit_def}')\n",
    "    for path_to_df in common_assays:\n",
    "        print(f'>>>>>>> CURRENT FILE : {path_to_df}')\n",
    "        path_to_df = path_to_df+'.csv'\n",
    "        set_file(prefer_args, path_to_df, limit_def)\n",
    "        data_info = {'path_to_data': path_to_dfs+path_to_df,\n",
    "                 'experiment_name': experiment_name,\n",
    "                 'id_column_name':id_column_name,\n",
    "                 'model_instance' : model_instance,\n",
    "                 'problem_type': problem_type,\n",
    "                 'smiles_column_name':smiles_column_name,\n",
    "                 'split_type': splitting_strategy,\n",
    "                 'temporal_info_column_name': temporal_info_column_name,\n",
    "                 'properties_column_name_list':properties_column_name, \n",
    "                'list_of_model_based_representations_paths': []}\n",
    "        try:\n",
    "            run_prefer_all(data_info, limit_def, prefer_args)\n",
    "        except Exception as e:\n",
    "            print(f'>>>>> Problem with file: {path_to_dfs+path_to_df} - in particular: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go into merged and mean\n",
    "import pandas as pd\n",
    "df_concat = pd.DataFrame()\n",
    "merged_mean = {}\n",
    "merged_std = {}\n",
    "for limit_def in limits_def:\n",
    "    path_to_mergeds = os.listdir(f'./merged_folder_limit_def_{limit_def}')\n",
    "    for merged in path_to_mergeds:\n",
    "        if(not merged.startswith('.')):\n",
    "            print((f'./merged_folder_limit_def_{limit_def}/{merged}'))\n",
    "            df = pd.read_csv(f'./merged_folder_limit_def_{limit_def}/{merged}')\n",
    "            df = df.iloc[3:]\n",
    "            df_concat = pd.concat((df, df_concat))\n",
    "        else:\n",
    "            continue\n",
    "        #collect all the deltaAUPRC for each merged table\n",
    "    df_concat.index = df_concat.Metrics\n",
    "    df_concat.drop(columns = ['Metrics'], inplace = True)\n",
    "    df_concat = df_concat.astype(float)\n",
    "    by_row_index = df_concat.groupby(df_concat.index)\n",
    "\n",
    "    merged_mean[limit_def] = by_row_index.mean() \n",
    "    merged_std[limit_def] = by_row_index.std() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (prefer-env-released2)",
   "language": "python",
   "name": "prefer-env-released2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
